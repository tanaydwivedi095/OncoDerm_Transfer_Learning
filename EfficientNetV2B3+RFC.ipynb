{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b6ee5-2f2d-46dd-af2e-88a587ec95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import pickle\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B3\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198ba12-a521-49ae-b5ce-eb20484272c8",
   "metadata": {},
   "source": [
    "### To be Changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c89896-1ad8-445b-bb2b-084f996debdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"D:\\\\Dataset 2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc31bae-c0bb-4931-8229-6dbb8fe8d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a29158-8afd-4587-ba9c-edb34a69a9d9",
   "metadata": {},
   "source": [
    "### To be Changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d5e4d-58f7-400a-9696-51e970dab813",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "for directory_path in glob.glob(\"D:\\\\Dataset 2\\\\train\\\\*\"):\n",
    "    label = directory_path.split(\"\\\\\")[-1]\n",
    "    print(label)\n",
    "    for img_path in glob.glob(os.path.join(directory_path,\"*.jpg\")):\n",
    "        print(img_path)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bbdbaa-bd8f-4eb8-8877-2eec95d05ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels_encoded = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35610d80-ae70-4b4f-8604-00da8e0e8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels_encoded, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61800fa1-4533-43c1-9354-e849568c330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X-Train\",x_train.shape)\n",
    "print(\"Y-Train\",y_train.shape)\n",
    "print(\"X-Test\",x_test.shape)\n",
    "print(\"Y-Test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ceb414-db8d-4116-a2df-85923db04bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 to 1\n",
    "x_train, x_test = x_train/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3690d0-8cff-4bc6-8362-0230a1edbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode y values for neural network\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13185f12-3882-4811-b864-cab814fbaa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model without classifier layers\n",
    "EfficientNet_model = EfficientNetV2B3(weights=\"imagenet\", include_top=False, input_shape = (SIZE,SIZE,3))\n",
    "EfficientNet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d2323-a883-436f-a3b2-41881af79343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in EfficientNet_model.layers:\n",
    "    layer.trainable = False\n",
    "EfficientNet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f063a7-64d7-4d68-94c7-a23ea6d2ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = EfficientNet_model.predict(x_train)\n",
    "features = feature_extractor.reshape(feature_extractor.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a932be-3294-4256-bea1-bde667d21fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_RF = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a3ece-93a4-4b9e-8d1c-46c167801544",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestClassifier(n_estimators=128, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc2f89-d0b9-44ba-9576-1ab5c8f5470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model.fit(X_for_RF, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c2071-2579-4f4c-8940-c677a2cbb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features = EfficientNet_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2cc0ce-87d1-409d-adad-46d8c44cbf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features = X_test_features.reshape(X_test_features.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5d1da-ec0f-4770-9ffd-7f0a8d544842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_RF = RF_model.predict(X_for_RF)\n",
    "prediction_RF = RF_model.predict(X_test_features)\n",
    "#prediction_RF = le.inverse_transform(prediction_RF)\n",
    "display(prediction_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901252f0-994b-421f-a0a4-5949b7a35936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy : {}\".format(metrics.accuracy_score(y_test, prediction_RF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27d448-f258-47c6-8fdb-a1b688c8851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, prediction_RF)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfc856-fe4e-465d-9952-0fa137ab995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randint(0, x_test.shape[0])\n",
    "img = x_test[n]\n",
    "#display(img)\n",
    "plt.imshow(img)\n",
    "input_img = np.expand_dims(img, axis=0)\n",
    "input_img_features = EfficientNet_model.predict(input_img)\n",
    "input_img_features = input_img_features.reshape(input_img_features.shape[0], -1)\n",
    "prediction_RF = RF_model.predict(input_img_features)[0]\n",
    "prediction_RF = le.inverse_transform([prediction_RF])\n",
    "print(\"The prediction for the image is \\t {}\".format(prediction_RF[0].title()))\n",
    "print(\"The actual label for the image is \\t {}\".format(le.inverse_transform([labels_encoded[n]])[0].title()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
